{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Wine Quality with k-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.4</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.029</td>\n",
       "      <td>31.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.98895</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.36</td>\n",
       "      <td>12.4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.7</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.29</td>\n",
       "      <td>14.9</td>\n",
       "      <td>0.053</td>\n",
       "      <td>55.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>0.99839</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0.52</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.31</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.045</td>\n",
       "      <td>34.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.99226</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.55</td>\n",
       "      <td>12.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.4</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.19</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.084</td>\n",
       "      <td>21.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>0.99516</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.63</td>\n",
       "      <td>9.6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.1</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.031</td>\n",
       "      <td>50.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>0.98950</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.43</td>\n",
       "      <td>12.7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            5.4              0.29         0.38             1.2      0.029   \n",
       "1            6.7              0.24         0.29            14.9      0.053   \n",
       "2            6.8              0.33         0.31             7.4      0.045   \n",
       "3            6.4              0.27         0.19             2.0      0.084   \n",
       "4            6.1              0.30         0.30             2.1      0.031   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 31.0                 132.0  0.98895  3.28       0.36   \n",
       "1                 55.0                 136.0  0.99839  3.03       0.52   \n",
       "2                 34.0                 143.0  0.99226  3.06       0.55   \n",
       "3                 21.0                 191.0  0.99516  3.49       0.63   \n",
       "4                 50.0                 163.0  0.98950  3.39       0.43   \n",
       "\n",
       "   alcohol  quality  \n",
       "0     12.4        6  \n",
       "1      9.0        5  \n",
       "2     12.2        6  \n",
       "3      9.6        4  \n",
       "4     12.7        7  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.read_csv(\"/Users/apple/Desktop/IC/spring semester/ML/assignment 1/sparklingwine.csv\")\n",
    "df = pd.read_csv(\"/Users/amermulla/Desktop/Imperial/Term 2/Machine Learning/Assignments/Assignment 1/Group Assignment/sparklingwine.csv\")\n",
    "df = df.drop(columns=[\"Unnamed: 0\"], errors=\"ignore\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create the binary column `good_wine`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"good_wine\"] = (df[\"quality\"] >= 6).astype(int)  # Binary column (1 if quality >= 6, else 0)\n",
    "\n",
    "X = df.drop(columns = [\"quality\", \"good_wine\"])     # Feature matrix\n",
    "y = df[\"good_wine\"]                                 # Target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When constructing the feature matrix `X`, `quality` and `good_wine` were both dropped because:\n",
    "\n",
    "- `good_wine` is the label we want to predict, so it should not appear in `X`\n",
    "- `good_wine` is defined directly from `quality`, so keeping `quality` as a feature would artificially inflate validation and test accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split the data into training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X.iloc[:900]\n",
    "y_train = y.iloc[:900]\n",
    "\n",
    "X_val = X.iloc[900:1200]\n",
    "y_val = y.iloc[900:1200]\n",
    "\n",
    "X_test = X.iloc[1200:]\n",
    "y_test = y.iloc[1200:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was split into training, validation, and test sets, without shuffling. The first 900 samples were used for training, the next 300 for validation, and the final 400 for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Normalise the features using the Z-score transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature matrix was normalised using Z-score normalisation (via `StandardScaler`). The scaler is fit on the training set only to learn each feature’s mean and standard deviation, and the same transformation is then applied to the validation and test sets to prevent data leakage. This step is important for k-Nearest Neighbours (k-NN) because the algorithm relies on Euclidean distances, so features on larger scales would otherwise dominate the distance calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train k-NN classifiers for k = 1, 2, …, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_accuracies = {}\n",
    "\n",
    "for k in range(1, 101):\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_val_pred = knn.predict(X_val_scaled)\n",
    "    acc = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "    validation_accuracies[k] = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A k-NN classifier was trained for each k from 1 to 100 using the training set, and each model was evaluated on the validation set using accuracy. The validation accuracy for each k was stored to select the best-performing model in the next step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Select the best k using the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k candidates: [1, 9, 17]\n",
      "Selected k: 1\n",
      "Validation accuracy: 0.7567\n"
     ]
    }
   ],
   "source": [
    "best_val_acc = max(validation_accuracies.values())\n",
    "best_ks = [k for k, v in validation_accuracies.items() if v == best_val_acc]\n",
    "best_k = min(best_ks)\n",
    "\n",
    "print(f\"Best k candidates: {best_ks}\")\n",
    "print(f\"Selected k: {best_k}\")\n",
    "print(f\"Validation accuracy: {best_val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum validation accuracy was achieved by multiple values of k (i.e., k = 1, 9, 17), so the smallest k achieving the maximum was selected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Predict the generalisation error using the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.6900\n",
      "Generalisation error: 0.3100\n"
     ]
    }
   ],
   "source": [
    "# Train best model on training data\n",
    "best_knn = KNeighborsClassifier(n_neighbors = best_k)\n",
    "best_knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Test prediction\n",
    "y_test_pred = best_knn.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "generalisation_error = 1 - test_accuracy\n",
    "\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Generalisation error: {generalisation_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selected value of k was evaluated on the test set to estimate the generalisation error. The test accuracy was lower than the validation accuracy, which is common when moving from model selection to evaluation on unseen data, and provides a more realistic estimate of generalisation performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for First Split:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.51      0.52       132\n",
      "           1       0.76      0.78      0.77       268\n",
      "\n",
      "    accuracy                           0.69       400\n",
      "   macro avg       0.65      0.64      0.65       400\n",
      "weighted avg       0.69      0.69      0.69       400\n",
      "\n",
      "Confusion Matrix for First Split:\n",
      "[[ 67  65]\n",
      " [ 59 209]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report for First Split:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "print(\"Confusion Matrix for First Split:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand performance beyond overall accuracy, a classification report and confusion matrix were computed on the test set. The results show stronger performance for class 1 (good wine) than class 0 (not good wine), which is consistent with the class imbalance in the test set (there are more class 1 instances). In particular, the model is less effective at identifying the minority class (class 0), so the overall accuracy can look reasonable even when performance on “not good wine” is relatively weak.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Evaluate k-NN under a new train/validation/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second split (train = 400, val = 400, test = 800)\n",
      "Best k candidates: [5, 6, 16]\n",
      "Selected k: 5\n",
      "Validation accuracy: 0.7375\n",
      "Test accuracy: 0.7475\n",
      "Generalisation error: 0.2525\n"
     ]
    }
   ],
   "source": [
    "# Second split\n",
    "X_train2 = X.iloc[:400]\n",
    "y_train2 = y.iloc[:400]\n",
    "\n",
    "X_val2 = X.iloc[400:800]\n",
    "y_val2 = y.iloc[400:800]\n",
    "\n",
    "X_test2 = X.iloc[800:]\n",
    "y_test2 = y.iloc[800:]\n",
    "\n",
    "# Normalisation\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train2_scaled = scaler.fit_transform(X_train2)\n",
    "X_val2_scaled = scaler.transform(X_val2)\n",
    "X_test2_scaled = scaler.transform(X_test2)\n",
    "\n",
    "# Train and validate\n",
    "validation_accuracies2 = {}\n",
    "\n",
    "for k in range(1, 101):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train2_scaled, y_train2)\n",
    "    \n",
    "    y_val_pred = knn.predict(X_val2_scaled)\n",
    "    acc = accuracy_score(y_val2, y_val_pred)\n",
    "    validation_accuracies2[k] = acc\n",
    "\n",
    "best_val_acc2 = max(validation_accuracies2.values())\n",
    "best_k2s = [k for k, v in validation_accuracies2.items() if v == best_val_acc2]\n",
    "best_k2 = min(best_k2s)\n",
    "\n",
    "# Train best model on training data\n",
    "best_knn2 = KNeighborsClassifier(n_neighbors = best_k2)\n",
    "best_knn2.fit(X_train2_scaled, y_train2)\n",
    "\n",
    "# Test prediction\n",
    "y_test2_pred = best_knn2.predict(X_test2_scaled)\n",
    "test_accuracy2 = accuracy_score(y_test2, y_test2_pred)\n",
    "generalisation_error2 = 1 - test_accuracy2\n",
    "\n",
    "print(\"Second split (train = 400, val = 400, test = 800)\")\n",
    "print(f\"Best k candidates: {best_k2s}\")\n",
    "print(f\"Selected k: {best_k2}\")\n",
    "print(f\"Validation accuracy: {best_val_acc2:.4f}\")\n",
    "print(f\"Test accuracy: {test_accuracy2:.4f}\")\n",
    "print(f\"Generalisation error: {generalisation_error2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the second split, k-NN models were trained for k = 1 to 100 and evaluated on the validation set. The maximum validation accuracy was achieved by multiple values of k (k = 5, 6, 16), so the smallest of these (k = 5) was selected. Evaluating this model on the test set, the test accuracy was 0.7475, corresponding to a generalisation error of 0.2525. Compared with the first split (test accuracy = 0.6900, generalisation error = 0.3100), the performance improved, suggesting that the choice of split and the amount of training data can noticeably affect the selected k and the estimated generalisation performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Second Split:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.58      0.60       259\n",
      "           1       0.81      0.83      0.82       541\n",
      "\n",
      "    accuracy                           0.75       800\n",
      "   macro avg       0.71      0.70      0.71       800\n",
      "weighted avg       0.74      0.75      0.75       800\n",
      "\n",
      "Confusion Matrix for Second Split:\n",
      "[[151 108]\n",
      " [ 94 447]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report for Second Split:\")\n",
    "print(classification_report(y_test2, y_test2_pred))\n",
    "\n",
    "print(\"Confusion Matrix for Second Split:\")\n",
    "print(confusion_matrix(y_test2, y_test2_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand performance beyond overall accuracy, a classification report and confusion matrix were computed on the test set (second split). The results again show stronger performance for class 1 (good wine) than class 0 (not good wine), which is consistent with the class imbalance in the test set (there are more class 1 instances). In particular, the model is less effective at identifying the minority class (class 0), so overall accuracy can look reasonable even when performance on “not good wine” is relatively weaker.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
