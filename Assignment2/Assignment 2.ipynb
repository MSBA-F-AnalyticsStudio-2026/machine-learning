{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50bec8d6",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "- Ankita Kokkera - 06032419\n",
    "- Aria Wang - 06047688\n",
    "- Tsamara Esperanti Erwin - 06042275\n",
    "- Jean-Marc Yao - 06055972\n",
    "- Amer Mulla - 06027165"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78464983",
   "metadata": {},
   "source": [
    "Import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e122ae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from naive_bayes import NaiveBayesForSpam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae13653b",
   "metadata": {},
   "source": [
    "1. Load the four dataset files into Python data frames and the two list files into lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a65dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_csv(\"training.txt\")\n",
    "validation = pd.read_csv(\"validation.txt\")\n",
    "test1 = pd.read_csv(\"test1.txt\")\n",
    "test2 = pd.read_csv(\"test2.txt\")\n",
    "\n",
    "with open(\"censored_list_test1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test1_list = f.read().splitlines()\n",
    "\n",
    "with open(\"censored_list_test2.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test2_list = f.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05bbb5b",
   "metadata": {},
   "source": [
    "2. Pre-process the SMS messages: Remove all punctuation and numbers from the SMS messages, and change all messages to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84aad4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [training, validation, test1, test2]:\n",
    "    df[\"sms\"] = (\n",
    "        df[\"sms\"]\n",
    "          .str.lower()\n",
    "          .str.replace(r\"[0-9]\", \"\", regex=True)\n",
    "          .str.replace(r\"[^\\w\\s]\", \"\", regex=True)\n",
    "          .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "          .str.strip()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23a9b6e",
   "metadata": {},
   "source": [
    "3. Naive Bayes Classifier from code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e022fc6b",
   "metadata": {},
   "source": [
    "4. Explain the code: What is the purpose of each function? What do train and train2\n",
    "do, and what is the difference between them? Where in the code is Bayesâ€™ Theorem\n",
    "being applied?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e988945",
   "metadata": {},
   "source": [
    "#### train\n",
    "The train function creates a comprehensive model that calculates likelihoods for every unique word found in the dataset.\n",
    "\n",
    "#### train2\n",
    "The train2 function acts as a filter that only retains \"spammy\" keywords. Specifically, â â€¯train2â€¯â  includes a conditional check that only saves a word if it is at least 20 times more likely to appear in a spam message than in a ham message. \n",
    "\n",
    "#### Difference between train and train2\n",
    "The main difference between train and train2 lies in the feature selection and the resulting size of the model's vocabulary. trainâ€¯â  maintains a full linguistic profile of both categories, including neutral and common words.â  train2â  results in a much smaller, specialized vocabulary focused on high-impact triggers, whereasâ â€¯\n",
    "\n",
    "#### predict\n",
    "The predict function applies Bayesâ€™ Theorem under the NaÃ¯ve Bayes independence assumption to compute posterior probabilities for ham and spam, and assigns the class with the higher posterior probability.\n",
    "\n",
    "#### score\n",
    "The score function evaluates the performance of the classifier on the dataset by computing prediction accuracy and the confusion matrix.\n",
    "\n",
    "#### Bayes Theorem\n",
    "The bayes theorem is applied in the for loop of predict function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961d6878",
   "metadata": {},
   "source": [
    "5. Use your training set to train the classifiers train and train2. Note that the interfaces\n",
    "of our classifiers require you to pass the ham and spam messages separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43f6f21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ham = training[training[\"label\"] == \"ham\"][\"sms\"].tolist()\n",
    "train_spam = training[training[\"label\"] == \"spam\"][\"sms\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2baa4f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_all = NaiveBayesForSpam()\n",
    "nb_all.train(train_ham, train_spam)\n",
    "\n",
    "nb_spam = NaiveBayesForSpam()\n",
    "nb_spam.train2(train_ham, train_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa367371",
   "metadata": {},
   "source": [
    "6. Using the validation set, explore how each of the two classifiers performs out of sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b61e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_labels = validation[\"label\"].tolist()\n",
    "validation_sms = validation[\"sms\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ae8f5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier using train():\n",
      "Accuracy: 0.955\n",
      "Confusion matrix:\n",
      " [[844.  29.]\n",
      " [ 16. 111.]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate train()\n",
    "acc_all, conf_all = nb_all.score(validation_sms, validation_labels)\n",
    "print(\"Classifier using train():\")\n",
    "print(\"Accuracy:\", acc_all)\n",
    "print(\"Confusion matrix:\\n\", conf_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87ad46d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier using train2():\n",
      "Accuracy: 0.963\n",
      "Confusion matrix:\n",
      " [[856.  33.]\n",
      " [  4. 107.]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate train2()\n",
    "acc_spam, conf_spam = nb_spam.score(validation_sms, validation_labels)\n",
    "print(\"Classifier using train2():\")\n",
    "print(\"Accuracy:\", acc_spam)\n",
    "print(\"Confusion matrix:\\n\", conf_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ec281a",
   "metadata": {},
   "source": [
    "7. Why is the train2 classifier faster? Why does it yield a better accuracy both on the training and the validation set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f8f117",
   "metadata": {},
   "source": [
    "The â train2â€¯â  classifier is faster because it drastically reduces the dimensionality of the model. In the first version, the algorithm must process every unique word in the entire dataset. In â train2â€¯â , however, the model discards any word that isn't at least 20 times more likely to be spam. This results in a much smaller list of â self.wordsâ . \n",
    "\n",
    "Train2 has better accuracy because it filters out the noises by Regarding the improved accuracy on both the training. Removal of Overlapping Features: In â â€¯train1â€¯â , common words like \"the,\" \"meeting,\" or \"hello\" might appear in both ham and spam.â â€¯train2â€¯â  ensures that only words with a very high signal-to-noise ratio are used for classification. Then it prevents overfitting to noise by ignoring neutral words that don't have a strong logical connection to spam, the model becomes more \"robust.\" Instead of getting distracted by the specific phrasing of a single email, it focuses on the undeniable markers of spam, which tend to be more consistent across both the training data and new, unseen validation data. It mitigates the independence assumption, Naive Bayes assumes all words are independent, which is rarely true in real language. By narrowing the feature set to only the most significant spam words, the model reduces the impact of this flawed assumption, focusing only on the variables that provide the most reliable evidence.\n",
    "\n",
    "#### Add feature selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2175e5a",
   "metadata": {},
   "source": [
    "8. How many false positives (ham messages classified as spam messages) did you get\n",
    "in your validation set? How would you change the code to reduce false positives at\n",
    "the expense of possibly having more false negatives (spam messages classified as ham\n",
    "messages)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c088c532",
   "metadata": {},
   "source": [
    "By using train(), the number of false positives in the validation set is 16. By using train2(), the number of false positives in the validation set is 4. \n",
    "The train2 classifier produces significantly fewer false positives than train, indicating that its feature selection strategy is more conservative in classifying ham messages as spam. To reduce false positives, we increase the decision threshold required to classify a message as spam. Instead of predicting spam when ð‘ƒ(spamâˆ£ð‘‹)>0.5, we require P(spamâˆ£X)>Ï„ with Ï„>0.5 (e.g., 0.8). This makes the classifier more conservative, lowering hamâ†’spam errors at the expense of potentially increasing spamâ†’ham errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da87652",
   "metadata": {},
   "source": [
    "9. Assuming the missing words are (Xj= xj , . . . , Xk = xk) with k â‰¤p, how would you\n",
    "change the formula given as hint to calculate P(Y= Cj |X1 = x1, . . . , Xp = xp) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4f9254",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e062a2ed",
   "metadata": {},
   "source": [
    "10. Modify the predict function in the code to implement the change in the previous point,\n",
    "and use it to report accuracies on test1, with both â€˜trainâ€™ and â€˜train2â€™."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3cd3f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check predict function\n",
    "\n",
    "class NaiveBayesForSpamNew:\n",
    "    \n",
    "    def train (self, hamMessages, spamMessages):\n",
    "        self.words = set (' '.join (hamMessages + spamMessages).split())\n",
    "        self.priors = np.zeros (2)\n",
    "        self.priors[0] = float (len (hamMessages)) / (len (hamMessages) + len (spamMessages))\n",
    "        self.priors[1] = 1.0 - self.priors[0]\n",
    "        self.likelihoods = []\n",
    "        for i, w in enumerate (self.words):\n",
    "            prob1 = (1.0 + len ([m for m in hamMessages if w in m])) / len (hamMessages)\n",
    "            prob2 = (1.0 + len ([m for m in spamMessages if w in m])) / len (spamMessages)\n",
    "            self.likelihoods.append ([min (prob1, 0.95), min (prob2, 0.95)])\n",
    "        self.likelihoods = np.array (self.likelihoods).T\n",
    "        \n",
    "    def train2 (self, hamMessages, spamMessages):\n",
    "        self.words = set (' '.join (hamMessages + spamMessages).split())\n",
    "        self.priors = np.zeros (2)\n",
    "        self.priors[0] = float (len (hamMessages)) / (len (hamMessages) + len (spamMessages))\n",
    "        self.priors[1] = 1.0 - self.priors[0]\n",
    "        self.likelihoods = []\n",
    "        spamkeywords = []\n",
    "        for i, w in enumerate (self.words):\n",
    "            prob1 = (1.0 + len ([m for m in hamMessages if w in m])) / len (hamMessages)\n",
    "            prob2 = (1.0 + len ([m for m in spamMessages if w in m])) / len (spamMessages)\n",
    "            if prob1 * 20 < prob2:\n",
    "                self.likelihoods.append ([min (prob1, 0.95), min (prob2, 0.95)])\n",
    "                spamkeywords.append (w)\n",
    "        self.words = spamkeywords\n",
    "        self.likelihoods = np.array (self.likelihoods).T\n",
    "\n",
    "    def predict(self, message, spam_threshold=0.8):\n",
    "        posteriors = np.copy(self.priors)\n",
    "        for i, w in enumerate(self.words):\n",
    "            if w in message.lower():\n",
    "                posteriors *= self.likelihoods[:, i]\n",
    "            else:\n",
    "                posteriors *= np.ones(2) - self.likelihoods[:, i]\n",
    "            posteriors = posteriors / np.linalg.norm(posteriors, ord=1)\n",
    "\n",
    "        # more conservative spam decision\n",
    "        if posteriors[1] >= spam_threshold:\n",
    "            return ['spam', posteriors[1]]\n",
    "        return ['ham', posteriors[0]]    \n",
    "\n",
    "    def score (self, messages, labels):\n",
    "        confusion = np.zeros(4).reshape (2,2)\n",
    "        for m, l in zip (messages, labels):\n",
    "            if self.predict(m)[0] == 'ham' and l == 'ham':\n",
    "                confusion[0,0] += 1\n",
    "            elif self.predict(m)[0] == 'ham' and l == 'spam':\n",
    "                confusion[0,1] += 1\n",
    "            elif self.predict(m)[0] == 'spam' and l == 'ham':\n",
    "                confusion[1,0] += 1\n",
    "            elif self.predict(m)[0] == 'spam' and l == 'spam':\n",
    "                confusion[1,1] += 1\n",
    "        return (confusion[0,0] + confusion[1,1]) / float (confusion.sum()), confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20cb72bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_all2 = NaiveBayesForSpamNew()\n",
    "nb_all2.train(train_ham, train_spam)\n",
    "\n",
    "nb_spam2 = NaiveBayesForSpamNew()\n",
    "nb_spam2.train2(train_ham, train_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a015780d",
   "metadata": {},
   "source": [
    "11. Repeat the previous point using test2. Briefly report your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4d205b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
