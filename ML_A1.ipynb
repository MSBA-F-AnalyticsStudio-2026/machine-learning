{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. loads the data file;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/apple/Desktop/IC/spring semester/ML/assignment 1/sparklingwine.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. construct a new binary column “good wine” that indicates whether the wine is good (which we define as having a quality of 6 or higher) or not;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"good_wine\"] = (df[\"quality\"] >= 6).astype(int)\n",
    "\n",
    "X = df.drop(columns=[\"quality\", \"good_wine\"]) \n",
    "y = df[\"good_wine\"] #classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the column quality and good wine from the database because good wine is the target variable and if we include quality as a feature, the model know the answer and validation and test accuracy will then be artificially inflated. We can not keep quality as a feature as well because binary_variable good wine was built based on the quality feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. splits the data set into a training data set (first 900 samples), a validation data set (next 300 samples) and a test data set (last 400 samples) — please do not shuffle the data, as it is already shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X.iloc[:900]\n",
    "y_train = y.iloc[:900]\n",
    "\n",
    "X_val = X.iloc[900:1200]\n",
    "y_val = y.iloc[900:1200]\n",
    "\n",
    "X_test = X.iloc[1200:]\n",
    "y_test = y.iloc[1200:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. normalises the data according to the Z-score transform;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was split into training, validation, and test sets. In the first experiment, the first 900 samples were used for training, the next 300 for validation, and the final 400 for testing. Z-score normalisation was then applied to the feature data. The mean and standard deviation were computed using the training set only, and the same transformation was applied to the validation and test sets to prevent data leakage. This step is important for k-Nearest Neighbours (k-NN), as the algorithm relies on distance calculations and is sensitive to feature scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. loads and trains the k-Nearest Neighbours classifiers for k= 1,2,...,100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_accuracies = {}\n",
    "\n",
    "for k in range(1, 101):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_val_pred = knn.predict(X_val_scaled)\n",
    "    acc = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "    validation_accuracies[k] = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. evaluates each classifier using the validation data set and selects the best classifier;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k: 7\n",
      "Validation accuracy: 0.9933\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Select best k\n",
    "best_k = max(validation_accuracies, key=validation_accuracies.get)\n",
    "best_val_acc = validation_accuracies[best_k]\n",
    "\n",
    "print(f\"Best k: {best_k}\")\n",
    "print(f\"Validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. predicts the generalisation error using the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9750\n",
      "Generalisation error: 0.0250\n"
     ]
    }
   ],
   "source": [
    "# Train best model on training data\n",
    "best_knn = KNeighborsClassifier(n_neighbors=best_k)\n",
    "best_knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Test prediction\n",
    "y_test_pred = best_knn.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "generalisation_error = 1 - test_accuracy\n",
    "\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Generalisation error: {generalisation_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For model training, k-NN classifiers were fitted for values of k=1 to 100 using the normalised training data. Each classifier was evaluated on the validation set, and the value of k that achieved the highest validation accuracy was selected as the best model. This approach balances model complexity, avoiding overfitting at very small values of k and underfitting at large values.\n",
    "\n",
    "The selected classifier was then evaluated on the test set to estimate the generalisation error. The test accuracy was lower than the training and validation performance, which is expected and indicates a realistic assessment of how well the model generalises to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for First Split:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96       132\n",
      "           1       0.97      0.99      0.98       268\n",
      "\n",
      "    accuracy                           0.97       400\n",
      "   macro avg       0.98      0.97      0.97       400\n",
      "weighted avg       0.98      0.97      0.97       400\n",
      "\n",
      "Confusion Matrix for First Split:\n",
      "[[124   8]\n",
      " [  2 266]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report for First Split:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "print(\"Confusion Matrix for First Split:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Try a new splitting: split the data set into a training data set (first 400 samples), a\n",
    "validation data set (next 400 samples), and a test data set (last 800 samples) - again,\n",
    "please do not shuffle the data. Then redo steps 4 to 7. What is the new generalisation\n",
    "error? Explain what you find.\n",
    "How do you judge whether the classifier is well-suited for the data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best k: 1\n",
      "New test accuracy: 0.9637\n",
      "New generalisation error: 0.0363\n"
     ]
    }
   ],
   "source": [
    "# Second split\n",
    "X_train2 = X.iloc[:400]\n",
    "y_train2 = y.iloc[:400]\n",
    "\n",
    "X_val2 = X.iloc[400:800]\n",
    "y_val2 = y.iloc[400:800]\n",
    "\n",
    "X_test2 = X.iloc[800:]\n",
    "y_test2 = y.iloc[800:]\n",
    "\n",
    "# Normalisation\n",
    "X_train2_scaled = scaler.fit_transform(X_train2)\n",
    "X_val2_scaled = scaler.transform(X_val2)\n",
    "X_test2_scaled = scaler.transform(X_test2)\n",
    "\n",
    "# Train and validate again\n",
    "validation_accuracies2 = {}\n",
    "\n",
    "for k in range(1, 101):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train2_scaled, y_train2)\n",
    "    \n",
    "    y_val_pred = knn.predict(X_val2_scaled)\n",
    "    acc = accuracy_score(y_val2, y_val_pred)\n",
    "    validation_accuracies2[k] = acc\n",
    "\n",
    "best_k2 = max(validation_accuracies2, key=validation_accuracies2.get)\n",
    "best_val_acc2 = validation_accuracies2[best_k2]\n",
    "\n",
    "# Test\n",
    "best_knn2 = KNeighborsClassifier(n_neighbors=best_k2)\n",
    "best_knn2.fit(X_train2_scaled, y_train2)\n",
    "\n",
    "y_test2_pred = best_knn2.predict(X_test2_scaled)\n",
    "test_accuracy2 = accuracy_score(y_test2, y_test2_pred)\n",
    "generalisation_error2 = 1 - test_accuracy2\n",
    "\n",
    "print(f\"New best k: {best_k2}\")\n",
    "print(f\"New test accuracy: {test_accuracy2:.4f}\")\n",
    "print(f\"New generalisation error: {generalisation_error2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new generalisation error is 0.0363 and is higher than the generalisation error before, which is 0.025. The new best k is 1, and if we change the range of k, the new best k is often the smallest value in the loop. This may because that with small training dataset, k always votes for itself, which then lead to overfitting.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94       259\n",
      "           1       0.98      0.97      0.97       541\n",
      "\n",
      "    accuracy                           0.96       800\n",
      "   macro avg       0.96      0.96      0.96       800\n",
      "weighted avg       0.96      0.96      0.96       800\n",
      "\n",
      "Confusion Matrix:\n",
      "[[246  13]\n",
      " [ 16 525]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test2, y_test2_pred))\n",
    "\n",
    "# --- Add confusion matrix ---\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test2, y_test2_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second experiment, the data was re-split into 400 training samples, 400 validation samples, and 800 test samples, and the same normalisation, training, and model selection procedure was repeated. The generalisation error increased compared to the first split. This is explained by the reduced size of the training set, which limits the model’s ability to learn reliable neighbourhood structures.\n",
    "\n",
    "Overall, k-NN performs reasonably well on this dataset after normalisation, but its performance is sensitive to the amount of training data and the choice of \n",
    "k. This suggests that while k-NN is suitable for this problem, its effectiveness depends strongly on data availability and proper preprocessing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
